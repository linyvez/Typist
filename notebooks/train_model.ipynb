{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "6f5r66pLef8o",
                "outputId": "cd304ab8-7418-4838-9f50-df507fa49613"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting evaluate\n",
                        "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
                        "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
                        "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
                        "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
                        "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
                        "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
                        "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
                        "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
                        "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
                        "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
                        "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
                        "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
                        "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
                        "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
                        "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
                        "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
                        "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
                        "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
                        "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
                        "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
                        "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
                        "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
                        "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
                        "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
                        "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
                        "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
                        "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hInstalling collected packages: evaluate\n",
                        "Successfully installed evaluate-0.4.6\n"
                    ]
                }
            ],
            "source": [
                "!pip install evaluate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "3Hveipr0TEMC",
                "outputId": "dbe3eab1-306a-49c7-8da9-02265cb4a894"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting torchcodec\n",
                        "  Downloading torchcodec-0.8.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
                        "Downloading torchcodec-0.8.1-cp312-cp312-manylinux_2_28_x86_64.whl (2.0 MB)\n",
                        "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.9/2.0 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hInstalling collected packages: torchcodec\n",
                        "Successfully installed torchcodec-0.8.1\n"
                    ]
                }
            ],
            "source": [
                "!pip install torchcodec"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "sjL9nKbycylz"
            },
            "outputs": [],
            "source": [
                "from torchaudio.datasets import LIBRISPEECH\n",
                "from pathlib import Path\n",
                "import torchaudio\n",
                "from transformers import (\n",
                "    Wav2Vec2ForCTC,\n",
                "    Wav2Vec2Processor,\n",
                "    Trainer,\n",
                "    TrainingArguments,\n",
                "    logging\n",
                ")\n",
                "from torch.utils.data import Dataset, ConcatDataset\n",
                "import evaluate\n",
                "import torch\n",
                "import warnings\n",
                "import glob\n",
                "import os\n",
                "import shutil\n",
                "\n",
                "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
                "logging.set_verbosity_error()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "F1JQtDGuc8cN",
                "outputId": "afd88a87-7210-453d-d937-260d2c7dbe08"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 5.95G/5.95G [05:41<00:00, 18.7MB/s]\n",
                        "100%|██████████| 322M/322M [00:22<00:00, 15.3MB/s]\n"
                    ]
                }
            ],
            "source": [
                "# loading dataset\n",
                "root = Path(\"data/raw/LIBRISPEECH\")\n",
                "root.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "train_ds = LIBRISPEECH(root=root, url=\"train-clean-100\", download=True)\n",
                "eval_ds = LIBRISPEECH(root=root, url=\"dev-clean\", download=True)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 177
                },
                "id": "5niIlcGsdCIt",
                "outputId": "2cfb90b3-620e-4112-e904-f37d6b6dcc20"
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ca9290f298ee433aa440e40da8668168",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5353c13751db432a945fcb19e26d718c",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3c05ac06e41c4ae2a4dadd9c280665c7",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "10699e1f36924ef686d8224a91dc3739",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "68806e2b1a6b4fa793d30fc0b27efa19",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# tokenizing transcripts\n",
                "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
                "tokenizer = processor.tokenizer\n",
                "feature_extractor = processor.feature_extractor\n",
                "\n",
                "class LibriSpeechDataset(Dataset):\n",
                "    def __init__(self, torchaudio_dataset, tokenizer):\n",
                "        self.dataset = torchaudio_dataset\n",
                "        self.tokenizer = tokenizer\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        data = self.dataset[idx]\n",
                "\n",
                "        if len(data) == 2:\n",
                "            waveform, sr = data\n",
                "            transcript = \"\"\n",
                "        else:\n",
                "            waveform, sr, transcript, *_ = data\n",
                "\n",
                "        if sr != 16000:\n",
                "            waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
                "\n",
                "        input_values = waveform.squeeze(0).numpy()\n",
                "\n",
                "        return {\"input_values\": input_values, \"labels\": transcript}\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.dataset)\n",
                "\n",
                "\n",
                "train_dataset = LibriSpeechDataset(train_ds, tokenizer)\n",
                "eval_dataset = LibriSpeechDataset(eval_ds, tokenizer)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 81
                },
                "id": "zve6Kq9ndFAQ",
                "outputId": "49bcca63-083f-44d9-e42d-fc4933647731"
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "8d1de3bf59d2423f8036f5f2f8e15793",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "265a4ff11b9e41479c969ea313d5b7d1",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# initializing wav2vec2 model\n",
                "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base\",\n",
                "                                       pad_token_id=processor.tokenizer.pad_token_id,\n",
                "                                       vocab_size=len(processor.tokenizer),\n",
                "                                       ctc_loss_reduction=\"mean\"\n",
                "                                       )\n",
                "model.freeze_feature_encoder()\n",
                "\n",
                "def data_collator(batch):\n",
                "    audio = [b[\"input_values\"] for b in batch]\n",
                "    text = [b[\"labels\"] for b in batch]\n",
                "\n",
                "    inputs = feature_extractor(\n",
                "        audio,\n",
                "        sampling_rate=16000,\n",
                "        padding=True,\n",
                "        return_attention_mask=True,\n",
                "        return_tensors=\"pt\"\n",
                "    )\n",
                "\n",
                "    labels_batch = tokenizer(\n",
                "        text,\n",
                "        padding=True,\n",
                "        return_tensors=\"pt\",\n",
                "        add_special_tokens=False\n",
                "    )\n",
                "\n",
                "    labels = labels_batch.input_ids\n",
                "    labels[labels == tokenizer.pad_token_id] = -100\n",
                "\n",
                "    return {\n",
                "        \"input_values\": inputs[\"input_values\"],\n",
                "        \"attention_mask\": inputs[\"attention_mask\"],\n",
                "        \"labels\": labels\n",
                "    }\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 223
                },
                "id": "UwXkeQzbhoVZ",
                "outputId": "254777b5-928b-4df1-e263-ba30700999d6"
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5685e05f91a9481ea63030d389d8dc07",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/380M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting jiwer\n",
                        "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
                        "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.1)\n",
                        "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
                        "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
                        "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
                        "Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
                        "Successfully installed jiwer-4.0.0 rapidfuzz-3.14.3\n"
                    ]
                }
            ],
            "source": [
                "!pip install --no-cache-dir jiwer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000
                },
                "collapsed": true,
                "id": "42nIMf1OdItW",
                "outputId": "a778114c-5cfd-41b0-b60b-65a2918f09b1"
            },
            "outputs": [],
            "source": [
                "# training, fine-tuning\n",
                "\n",
                "wer_metric = evaluate.load(\"wer\")\n",
                "cer_metric = evaluate.load(\"cer\")\n",
                "\n",
                "def compute_metrics(pred):\n",
                "    pred_logits = pred.predictions\n",
                "    pred_ids = torch.argmax(torch.tensor(pred_logits), dim=-1)\n",
                "\n",
                "    # decode\n",
                "    pred_str = tokenizer.batch_decode(pred_ids)\n",
                "    label_ids = pred.label_ids\n",
                "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
                "    label_str = tokenizer.batch_decode(label_ids)\n",
                "\n",
                "    print(\"\\n\" + \"=\"*30)\n",
                "    print(f\"SAMPLE 1 TARGET: {label_str[0]}\")\n",
                "    print(f\"SAMPLE 1 PRED:   {pred_str[0]}\")\n",
                "    print(\"-\" * 10)\n",
                "    print(f\"SAMPLE 2 TARGET: {label_str[1]}\")\n",
                "    print(f\"SAMPLE 2 PRED:   {pred_str[1]}\")\n",
                "    print(\"=\"*30 + \"\\n\")\n",
                "\n",
                "    return {\n",
                "        \"wer\": wer_metric.compute(predictions=pred_str, references=label_str),\n",
                "        \"cer\": cer_metric.compute(predictions=pred_str, references=label_str)\n",
                "    }\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./results\",\n",
                "    eval_strategy=\"steps\",\n",
                "    eval_steps=500,\n",
                "    save_strategy=\"steps\",\n",
                "    save_steps=500,\n",
                "    save_total_limit=2,\n",
                "    learning_rate=1e-4,\n",
                "    per_device_train_batch_size=16,\n",
                "    per_device_eval_batch_size=16,\n",
                "    num_train_epochs=10,\n",
                "    warmup_steps=1000,\n",
                "    logging_steps=100,\n",
                "    fp16=torch.cuda.is_available(),\n",
                "    max_grad_norm=1.0,\n",
                "    gradient_accumulation_steps=2,\n",
                "    report_to=[],\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_dataset,\n",
                "    eval_dataset=eval_dataset,\n",
                "    data_collator=data_collator,\n",
                "    compute_metrics=compute_metrics,\n",
                ")\n",
                "\n",
                "trainer.train()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "1HEO5V7BdLPP",
                "outputId": "44b54079-fd19-4278-e786-d0865c13ce9d"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==============================\n",
                        "SAMPLE 1 TARGET: MISTER QUILTER IS THE APOSTLE OF THE MIDLE CLASES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n",
                        "SAMPLE 1 PRED:   MISTER QUILER IS THE OPPOSAL OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n",
                        "----------\n",
                        "SAMPLE 2 TARGET: NOR IS MISTER QUILTER'S MANER LES INTERESTING THAN HIS MATER\n",
                        "SAMPLE 2 PRED:   NOR IS MISTER QUILTER'S MANNER LESS INTERESTING THAN HIS MATTER\n",
                        "==============================\n",
                        "\n",
                        "{'eval_loss': 0.10452383756637573, 'eval_wer': 0.16186904893202456, 'eval_cer': 0.039728733536182406, 'eval_runtime': 127.8705, 'eval_samples_per_second': 21.139, 'eval_steps_per_second': 1.322, 'epoch': 3.368834080717489}\n",
                        "Validation WER: 0.1619\n",
                        "Validation CER: 0.0397\n"
                    ]
                }
            ],
            "source": [
                "# metrics\n",
                "eval_results = trainer.evaluate()\n",
                "print(f\"Validation WER: {eval_results['eval_wer']:.4f}\")\n",
                "print(f\"Validation CER: {eval_results['eval_cer']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "bqYYHMn0eBpH",
                "outputId": "d8f5373c-bf2d-4dc6-8714-3425f6499b99"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 331M/331M [00:19<00:00, 17.7MB/s]\n"
                    ]
                }
            ],
            "source": [
                "test_dataset = LIBRISPEECH(root=root, url=\"test-clean\", download=True)\n",
                "test_dataset = LibriSpeechDataset(test_dataset, tokenizer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "ql3ItyifeMBF",
                "outputId": "5dfea2f1-ae0c-4afd-b5ea-c8ab5e09fdb2"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==============================\n",
                        "SAMPLE 1 TARGET: HE HOPED THERE WOULD BE STEW FOR DINER TURNIPS AND CAROTS AND BRUISED POTATOES AND FAT MUTON PIECES TO BE LADLED OUT IN THICK PEPERED FLOUR FATENED SAUCE\n",
                        "SAMPLE 1 PRED:   HE HOPED THERE WOULD BE STE FOR DINNER TURNIPS AND CARRETS AND BRUISED POTATOES AND FAT MUTTEN PIECES TO BE LAIDLED OUT IN THICK PEPPERED FLOWER FATTINED SAUCE\n",
                        "----------\n",
                        "SAMPLE 2 TARGET: STUF IT INTO YOU HIS BELY COUNSELED HIM\n",
                        "SAMPLE 2 PRED:   STUFF IT INTO YOU HIS BELLY COUNCELED HIM\n",
                        "==============================\n",
                        "\n",
                        "{'eval_loss': 0.11569350212812424, 'eval_wer': 0.16172778454047473, 'eval_cer': 0.03994150310945724, 'eval_runtime': 130.2868, 'eval_samples_per_second': 20.109, 'eval_steps_per_second': 1.259, 'epoch': 3.368834080717489}\n",
                        "Test WER: 0.1617\n",
                        "Test CER: 0.0399\n"
                    ]
                }
            ],
            "source": [
                "test_results = trainer.evaluate(test_dataset)\n",
                "print(f\"Test WER: {test_results['eval_wer']:.4f}\")\n",
                "print(f\"Test CER: {test_results['eval_cer']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trainer.save_model(\"./pretrained_model\")\n",
                "processor.save_pretrained(\"./pretrained_model\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "shutil.make_archive('Wav2Vec2-base-LibriSpeech100h', 'zip', \"./pretrained_model\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Domain adaptation: training on custom datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CommandDataset(Dataset):\n",
                "    def __init__(self, folder_path, tokenizer, override_label=None):\n",
                "        self.files = glob.glob(os.path.join(folder_path, \"*.flac\"))\n",
                "        self.tokenizer = tokenizer\n",
                "        self.override_label = override_label\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        file_path = self.files[idx]\n",
                "\n",
                "        if self.override_label:\n",
                "            transcript = self.override_label\n",
                "        else:\n",
                "            filename = os.path.basename(file_path)\n",
                "            clean_name = filename.replace(\".flac\", \"\")\n",
                "            text_part = \"_\".join(clean_name.split(\"_\")[:-1])\n",
                "            text_part = text_part.replace(\"dynamic_\", \"\")\n",
                "            transcript = text_part.replace(\"_\", \" \").upper()\n",
                "\n",
                "        waveform, sr = torchaudio.load(file_path)\n",
                "        \n",
                "        if sr != 16000:\n",
                "            waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
                "\n",
                "        input_values = waveform.squeeze(0).numpy()\n",
                "\n",
                "        return {\"input_values\": input_values, \"labels\": transcript}\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.files)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if not os.path.exists(\"data/raw/custom/commands_dataset\"):\n",
                "    shutil.unpack_archive(\"commands_dataset.zip\", \"data/raw/custom/commands_dataset\")\n",
                "\n",
                "if not os.path.exists(\"data/raw/custom/wakeup_dataset\"):\n",
                "    shutil.unpack_archive(\"wakeup_dataset.zip\", \"data/raw/custom/wakeup_dataset\")\n",
                "\n",
                "if not os.path.exists(\"results/Wav2Vec2-base-LibriSpeech100h\"):\n",
                "    shutil.unpack_archive(\"Wav2Vec2-base-LibriSpeech100h.zip\", \"results/Wav2Vec2-base-LibriSpeech100h\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trained_processor = Wav2Vec2Processor.from_pretrained(\"Wav2Vec2-base-LibriSpeech100h\")\n",
                "trained_tokenizer = trained_processor.tokenizer\n",
                "trained_feature_extractor = trained_processor.feature_extractor\n",
                "\n",
                "trained_model = Wav2Vec2ForCTC.from_pretrained(\"Wav2Vec2-base-LibriSpeech100h\")\n",
                "trained_model.freeze_feature_encoder()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "command_dataset = CommandDataset(\"data/raw/custom/commands_dataset\", trained_tokenizer)\n",
                "wakeup_dataset = CommandDataset(\"data/raw/custom/wakeup_dataset\", trained_tokenizer, override_label=\"WAKE UP TYPIST\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "librispeech_subset = torch.utils.data.Subset(train_ds, torch.randperm(len(train_ds))[:1000])\n",
                "subset_dataset = LibriSpeechDataset(librispeech_subset, trained_tokenizer)\n",
                "\n",
                "train_domain_adapted = ConcatDataset([subset_dataset] + [command_dataset] * 2 + [wakeup_dataset] * 2)\n",
                "print(f\"Domain adapted train dataset length: {len(train_domain_adapted)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "eval_size = int(0.1 * len(train_domain_adapted))\n",
                "train_adapted_ds, eval_adapted_ds = torch.utils.data.random_split(train_domain_adapted, [len(train_domain_adapted)-eval_size, eval_size])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def data_collator_adapted(batch):\n",
                "    audio = [b[\"input_values\"] for b in batch]\n",
                "    text = [b[\"labels\"] for b in batch]\n",
                "\n",
                "    inputs = trained_feature_extractor(\n",
                "        audio,\n",
                "        sampling_rate=16000,\n",
                "        padding=True,\n",
                "        return_attention_mask=True,\n",
                "        return_tensors=\"pt\"\n",
                "    )\n",
                "\n",
                "    labels_batch = trained_tokenizer(\n",
                "        text,\n",
                "        padding=True,\n",
                "        return_tensors=\"pt\",\n",
                "        add_special_tokens=False\n",
                "    )\n",
                "\n",
                "    labels = labels_batch.input_ids\n",
                "    labels[labels == trained_tokenizer.pad_token_id] = -100\n",
                "\n",
                "    return {\n",
                "        \"input_values\": inputs[\"input_values\"],\n",
                "        \"attention_mask\": inputs[\"attention_mask\"],\n",
                "        \"labels\": labels\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wer_metric = evaluate.load(\"wer\")\n",
                "cer_metric = evaluate.load(\"cer\")\n",
                "\n",
                "def compute_metrics_adapted(pred):\n",
                "    pred_logits = pred.predictions\n",
                "    pred_ids = torch.argmax(torch.tensor(pred_logits), dim=-1)\n",
                "\n",
                "    # decode\n",
                "    pred_str = trained_tokenizer.batch_decode(pred_ids)\n",
                "    label_ids = pred.label_ids\n",
                "    label_ids[label_ids == -100] = trained_tokenizer.pad_token_id\n",
                "    label_str = trained_tokenizer.batch_decode(label_ids, group_tokens=False)\n",
                "\n",
                "    print(f\"\\nTARGET: {label_str[0]} \\nPRED: {pred_str[0]}\")\n",
                "\n",
                "    return {\n",
                "        \"wer\": wer_metric.compute(predictions=pred_str, references=label_str),\n",
                "        \"cer\": cer_metric.compute(predictions=pred_str, references=label_str)\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.environ[\"WANDB_DISABLED\"] = \"true\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "training_args_adapted = TrainingArguments(\n",
                "    output_dir=\"./finetuned_results\",\n",
                "    eval_strategy=\"steps\",\n",
                "    eval_steps=100,\n",
                "    save_strategy=\"steps\",\n",
                "    save_steps=100,\n",
                "    learning_rate=2e-5,\n",
                "    per_device_train_batch_size=8,\n",
                "    num_train_epochs=3,\n",
                "    warmup_steps=100,\n",
                "    fp16=torch.cuda.is_available(),\n",
                "    logging_steps=50,\n",
                "    save_total_limit=2,\n",
                ")\n",
                "\n",
                "trainer_adapted = Trainer(\n",
                "    model=trained_model,\n",
                "    args=training_args_adapted,\n",
                "    train_dataset=train_adapted_ds,\n",
                "    eval_dataset=eval_adapted_ds,\n",
                "    data_collator=data_collator_adapted,\n",
                "    compute_metrics=compute_metrics_adapted\n",
                ")\n",
                "\n",
                "trainer_adapted.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trainer_adapted.save_model(\"./final_model\")\n",
                "trained_tokenizer.save_pretrained(\"./final_model\")\n",
                "trained_feature_extractor.save_pretrained(\"./final_model\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "shutil.make_archive('Wav2Vec2-base-LibriSpeech100h-Custom', 'zip', \"./final_model\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
