{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "6f5r66pLef8o",
                "outputId": "cd304ab8-7418-4838-9f50-df507fa49613"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting evaluate\n",
                        "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
                        "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
                        "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
                        "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
                        "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
                        "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
                        "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
                        "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
                        "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
                        "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
                        "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
                        "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
                        "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
                        "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
                        "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
                        "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
                        "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
                        "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
                        "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
                        "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
                        "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
                        "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
                        "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
                        "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
                        "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
                        "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
                        "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hInstalling collected packages: evaluate\n",
                        "Successfully installed evaluate-0.4.6\n"
                    ]
                }
            ],
            "source": [
                "!pip install evaluate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "3Hveipr0TEMC",
                "outputId": "dbe3eab1-306a-49c7-8da9-02265cb4a894"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting torchcodec\n",
                        "  Downloading torchcodec-0.8.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
                        "Downloading torchcodec-0.8.1-cp312-cp312-manylinux_2_28_x86_64.whl (2.0 MB)\n",
                        "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.9/2.0 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hInstalling collected packages: torchcodec\n",
                        "Successfully installed torchcodec-0.8.1\n"
                    ]
                }
            ],
            "source": [
                "!pip install torchcodec"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "sjL9nKbycylz"
            },
            "outputs": [],
            "source": [
                "from torchaudio.datasets import LIBRISPEECH\n",
                "from pathlib import Path\n",
                "import torchaudio\n",
                "from transformers import (\n",
                "    Wav2Vec2ForCTC,\n",
                "    Wav2Vec2Processor,\n",
                "    Trainer,\n",
                "    TrainingArguments,\n",
                "    logging\n",
                ")\n",
                "from torch.utils.data import Dataset, ConcatDataset\n",
                "import evaluate\n",
                "import torch\n",
                "import warnings\n",
                "import glob\n",
                "import os\n",
                "import shutil\n",
                "\n",
                "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
                "logging.set_verbosity_error()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "F1JQtDGuc8cN",
                "outputId": "afd88a87-7210-453d-d937-260d2c7dbe08"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 5.95G/5.95G [05:41<00:00, 18.7MB/s]\n",
                        "100%|██████████| 322M/322M [00:22<00:00, 15.3MB/s]\n"
                    ]
                }
            ],
            "source": [
                "# loading dataset\n",
                "root = Path(\"data/raw/LIBRISPEECH\")\n",
                "root.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "train_ds = LIBRISPEECH(root=root, url=\"train-clean-100\", download=True)\n",
                "eval_ds = LIBRISPEECH(root=root, url=\"dev-clean\", download=True)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 177
                },
                "id": "5niIlcGsdCIt",
                "outputId": "2cfb90b3-620e-4112-e904-f37d6b6dcc20"
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ca9290f298ee433aa440e40da8668168",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5353c13751db432a945fcb19e26d718c",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3c05ac06e41c4ae2a4dadd9c280665c7",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "10699e1f36924ef686d8224a91dc3739",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "68806e2b1a6b4fa793d30fc0b27efa19",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# tokenizing transcripts\n",
                "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
                "tokenizer = processor.tokenizer\n",
                "feature_extractor = processor.feature_extractor\n",
                "\n",
                "class LibriSpeechDataset(Dataset):\n",
                "    def __init__(self, torchaudio_dataset, tokenizer):\n",
                "        self.dataset = torchaudio_dataset\n",
                "        self.tokenizer = tokenizer\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        data = self.dataset[idx]\n",
                "\n",
                "        if len(data) == 2:\n",
                "            waveform, sr = data\n",
                "            transcript = \"\"\n",
                "        else:\n",
                "            waveform, sr, transcript, *_ = data\n",
                "\n",
                "        if sr != 16000:\n",
                "            waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
                "\n",
                "        input_values = waveform.squeeze(0).numpy()\n",
                "\n",
                "        return {\"input_values\": input_values, \"labels\": transcript}\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.dataset)\n",
                "\n",
                "\n",
                "train_dataset = LibriSpeechDataset(train_ds, tokenizer)\n",
                "eval_dataset = LibriSpeechDataset(eval_ds, tokenizer)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 81
                },
                "id": "zve6Kq9ndFAQ",
                "outputId": "49bcca63-083f-44d9-e42d-fc4933647731"
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "8d1de3bf59d2423f8036f5f2f8e15793",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "265a4ff11b9e41479c969ea313d5b7d1",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# initializing wav2vec2 model\n",
                "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base\",\n",
                "                                       pad_token_id=processor.tokenizer.pad_token_id,\n",
                "                                       vocab_size=len(processor.tokenizer),\n",
                "                                       ctc_loss_reduction=\"mean\"\n",
                "                                       )\n",
                "model.freeze_feature_encoder()\n",
                "\n",
                "def data_collator(batch):\n",
                "    audio = [b[\"input_values\"] for b in batch]\n",
                "    text = [b[\"labels\"] for b in batch]\n",
                "\n",
                "    inputs = feature_extractor(\n",
                "        audio,\n",
                "        sampling_rate=16000,\n",
                "        padding=True,\n",
                "        return_attention_mask=True,\n",
                "        return_tensors=\"pt\"\n",
                "    )\n",
                "\n",
                "    labels_batch = tokenizer(\n",
                "        text,\n",
                "        padding=True,\n",
                "        return_tensors=\"pt\",\n",
                "        add_special_tokens=False\n",
                "    )\n",
                "\n",
                "    labels = labels_batch.input_ids\n",
                "    labels[labels == tokenizer.pad_token_id] = -100\n",
                "\n",
                "    return {\n",
                "        \"input_values\": inputs[\"input_values\"],\n",
                "        \"attention_mask\": inputs[\"attention_mask\"],\n",
                "        \"labels\": labels\n",
                "    }\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 223
                },
                "id": "UwXkeQzbhoVZ",
                "outputId": "254777b5-928b-4df1-e263-ba30700999d6"
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5685e05f91a9481ea63030d389d8dc07",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/380M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting jiwer\n",
                        "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
                        "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.1)\n",
                        "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
                        "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
                        "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
                        "Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
                        "Successfully installed jiwer-4.0.0 rapidfuzz-3.14.3\n"
                    ]
                }
            ],
            "source": [
                "!pip install --no-cache-dir jiwer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000
                },
                "collapsed": true,
                "id": "42nIMf1OdItW",
                "outputId": "a778114c-5cfd-41b0-b60b-65a2918f09b1"
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "990553ee87df4571ad45e7f1932187db",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading builder script: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "328c8157475842159047809ab768ed77",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading builder script: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'loss': 6.2921, 'grad_norm': 1.9881415367126465, 'learning_rate': 9.900000000000002e-06, 'epoch': 0.11210762331838565}\n",
                        "{'loss': 2.9822, 'grad_norm': 0.586137056350708, 'learning_rate': 1.9900000000000003e-05, 'epoch': 0.2242152466367713}\n",
                        "{'loss': 2.8968, 'grad_norm': 0.5415200591087341, 'learning_rate': 2.9900000000000002e-05, 'epoch': 0.336322869955157}\n",
                        "{'loss': 2.8719, 'grad_norm': 0.8823453187942505, 'learning_rate': 3.99e-05, 'epoch': 0.4484304932735426}\n",
                        "{'loss': 2.8645, 'grad_norm': 0.27644065022468567, 'learning_rate': 4.99e-05, 'epoch': 0.5605381165919282}\n",
                        "\n",
                        "==============================\n",
                        "SAMPLE 1 TARGET: MISTER QUILTER IS THE APOSTLE OF THE MIDLE CLASES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n",
                        "SAMPLE 1 PRED:   \n",
                        "----------\n",
                        "SAMPLE 2 TARGET: NOR IS MISTER QUILTER'S MANER LES INTERESTING THAN HIS MATER\n",
                        "SAMPLE 2 PRED:   \n",
                        "==============================\n",
                        "\n",
                        "{'eval_loss': 2.897073745727539, 'eval_wer': 1.0, 'eval_cer': 1.0, 'eval_runtime': 127.896, 'eval_samples_per_second': 21.134, 'eval_steps_per_second': 1.321, 'epoch': 0.5605381165919282}\n",
                        "{'loss': 2.5698, 'grad_norm': 2.5105342864990234, 'learning_rate': 5.99e-05, 'epoch': 0.672645739910314}\n",
                        "{'loss': 0.7734, 'grad_norm': 1.3306256532669067, 'learning_rate': 6.99e-05, 'epoch': 0.7847533632286996}\n",
                        "{'loss': 0.3674, 'grad_norm': 1.3015769720077515, 'learning_rate': 7.99e-05, 'epoch': 0.8968609865470852}\n",
                        "{'loss': 0.268, 'grad_norm': 0.8504197001457214, 'learning_rate': 8.99e-05, 'epoch': 1.0089686098654709}\n",
                        "{'loss': 0.2187, 'grad_norm': 1.6923441886901855, 'learning_rate': 9.99e-05, 'epoch': 1.1210762331838564}\n",
                        "\n",
                        "==============================\n",
                        "SAMPLE 1 TARGET: MISTER QUILTER IS THE APOSTLE OF THE MIDLE CLASES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n",
                        "SAMPLE 1 PRED:   MISTER QUILLTER IS THE OPOSSL OF THE MIDDL CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPLE\n",
                        "----------\n",
                        "SAMPLE 2 TARGET: NOR IS MISTER QUILTER'S MANER LES INTERESTING THAN HIS MATER\n",
                        "SAMPLE 2 PRED:   NOR IS MISTER QUILTER'S MANER LESS INTERESTING THAN HIS MATTER\n",
                        "==============================\n",
                        "\n",
                        "{'eval_loss': 0.18560591340065002, 'eval_wer': 0.20627918091246644, 'eval_cer': 0.05332032837519304, 'eval_runtime': 127.1426, 'eval_samples_per_second': 21.26, 'eval_steps_per_second': 1.329, 'epoch': 1.1210762331838564}\n",
                        "{'loss': 0.1873, 'grad_norm': 0.7784032225608826, 'learning_rate': 9.875000000000002e-05, 'epoch': 1.2331838565022422}\n",
                        "{'loss': 0.1753, 'grad_norm': 0.7577558159828186, 'learning_rate': 9.748737373737374e-05, 'epoch': 1.3452914798206277}\n",
                        "{'loss': 0.165, 'grad_norm': 0.7231572270393372, 'learning_rate': 9.622474747474748e-05, 'epoch': 1.4573991031390134}\n",
                        "{'loss': 0.1505, 'grad_norm': 0.8010507225990295, 'learning_rate': 9.496212121212121e-05, 'epoch': 1.5695067264573992}\n",
                        "{'loss': 0.1459, 'grad_norm': 0.7421630024909973, 'learning_rate': 9.369949494949496e-05, 'epoch': 1.6816143497757847}\n",
                        "\n",
                        "==============================\n",
                        "SAMPLE 1 TARGET: MISTER QUILTER IS THE APOSTLE OF THE MIDLE CLASES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n",
                        "SAMPLE 1 PRED:   MISTER QUILTER IS THE APPOSAL OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSSPAL\n",
                        "----------\n",
                        "SAMPLE 2 TARGET: NOR IS MISTER QUILTER'S MANER LES INTERESTING THAN HIS MATER\n",
                        "SAMPLE 2 PRED:   NOR IS MISTER QUUILTER'S MANNER LESS INTERRESTING THAN HIS MATERK\n",
                        "==============================\n",
                        "\n",
                        "{'eval_loss': 0.1356673687696457, 'eval_wer': 0.17900077203044007, 'eval_cer': 0.045425470634592485, 'eval_runtime': 130.7629, 'eval_samples_per_second': 20.671, 'eval_steps_per_second': 1.292, 'epoch': 1.6816143497757847}\n",
                        "{'loss': 0.1345, 'grad_norm': 0.5561276078224182, 'learning_rate': 9.243686868686869e-05, 'epoch': 1.7937219730941703}\n",
                        "{'loss': 0.1325, 'grad_norm': 0.7498613595962524, 'learning_rate': 9.117424242424243e-05, 'epoch': 1.905829596412556}\n",
                        "{'loss': 0.1263, 'grad_norm': 0.5498320460319519, 'learning_rate': 8.991161616161617e-05, 'epoch': 2.0179372197309418}\n",
                        "{'loss': 0.1036, 'grad_norm': 0.4685996472835541, 'learning_rate': 8.86489898989899e-05, 'epoch': 2.1300448430493275}\n",
                        "{'loss': 0.1038, 'grad_norm': 0.6187208890914917, 'learning_rate': 8.738636363636365e-05, 'epoch': 2.242152466367713}\n",
                        "\n",
                        "==============================\n",
                        "SAMPLE 1 TARGET: MISTER QUILTER IS THE APOSTLE OF THE MIDLE CLASES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n",
                        "SAMPLE 1 PRED:   MISTER QUILTER IS THE OPPOSSAL OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPELE\n",
                        "----------\n",
                        "SAMPLE 2 TARGET: NOR IS MISTER QUILTER'S MANER LES INTERESTING THAN HIS MATER\n",
                        "SAMPLE 2 PRED:   NOR IS MISTER QULKER'S MANNER LESS INTERESTING THAN HIS MATTER\n",
                        "==============================\n",
                        "\n",
                        "{'eval_loss': 0.1257764846086502, 'eval_wer': 0.17113341421271278, 'eval_cer': 0.042888090214829186, 'eval_runtime': 129.7535, 'eval_samples_per_second': 20.832, 'eval_steps_per_second': 1.302, 'epoch': 2.242152466367713}\n",
                        "{'loss': 0.1037, 'grad_norm': 0.6807224750518799, 'learning_rate': 8.612373737373738e-05, 'epoch': 2.3542600896860986}\n",
                        "{'loss': 0.0995, 'grad_norm': 0.8162177205085754, 'learning_rate': 8.486111111111112e-05, 'epoch': 2.4663677130044843}\n",
                        "{'loss': 0.0997, 'grad_norm': 0.8171247839927673, 'learning_rate': 8.359848484848484e-05, 'epoch': 2.57847533632287}\n",
                        "{'loss': 0.0964, 'grad_norm': 0.551742672920227, 'learning_rate': 8.23358585858586e-05, 'epoch': 2.6905829596412554}\n",
                        "{'loss': 0.0982, 'grad_norm': 0.6054419875144958, 'learning_rate': 8.107323232323232e-05, 'epoch': 2.802690582959641}\n",
                        "\n",
                        "==============================\n",
                        "SAMPLE 1 TARGET: MISTER QUILTER IS THE APOSTLE OF THE MIDLE CLASES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n",
                        "SAMPLE 1 PRED:   MISTER QUILTER IS THE OPPOSSAL OF THE MIDDLE CLASSES AND WE'ARE GLAD TO WELCOME HIS GOSPEL\n",
                        "----------\n",
                        "SAMPLE 2 TARGET: NOR IS MISTER QUILTER'S MANER LES INTERESTING THAN HIS MATER\n",
                        "SAMPLE 2 PRED:   NOR IS MISTER QUILTER'S MANNER LESS INTERESTING THAN HIS MATTERC\n",
                        "==============================\n",
                        "\n",
                        "{'eval_loss': 0.11217480152845383, 'eval_wer': 0.1649204073379655, 'eval_cer': 0.04099035583403129, 'eval_runtime': 127.3416, 'eval_samples_per_second': 21.226, 'eval_steps_per_second': 1.327, 'epoch': 2.802690582959641}\n",
                        "{'loss': 0.0969, 'grad_norm': 0.4486183226108551, 'learning_rate': 7.981060606060606e-05, 'epoch': 2.914798206278027}\n",
                        "{'loss': 0.0883, 'grad_norm': 0.474605530500412, 'learning_rate': 7.85479797979798e-05, 'epoch': 3.0269058295964126}\n",
                        "{'loss': 0.0782, 'grad_norm': 0.6315237283706665, 'learning_rate': 7.728535353535354e-05, 'epoch': 3.1390134529147984}\n",
                        "{'loss': 0.0792, 'grad_norm': 0.4186270236968994, 'learning_rate': 7.602272727272728e-05, 'epoch': 3.2511210762331837}\n",
                        "{'loss': 0.0788, 'grad_norm': 1.0213704109191895, 'learning_rate': 7.476010101010101e-05, 'epoch': 3.3632286995515694}\n",
                        "\n",
                        "==============================\n",
                        "SAMPLE 1 TARGET: MISTER QUILTER IS THE APOSTLE OF THE MIDLE CLASES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n",
                        "SAMPLE 1 PRED:   MISTER QUILTER IS THE OPPOSEL OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n",
                        "----------\n",
                        "SAMPLE 2 TARGET: NOR IS MISTER QUILTER'S MANER LES INTERESTING THAN HIS MATER\n",
                        "SAMPLE 2 PRED:   NOR IS MISTER QUILTER'S MANNER LESS INTERESTING THAN HIS MATTER\n",
                        "==============================\n",
                        "\n",
                        "{'eval_loss': 0.10677024722099304, 'eval_wer': 0.16192419396345722, 'eval_cer': 0.03958030738349431, 'eval_runtime': 126.919, 'eval_samples_per_second': 21.297, 'eval_steps_per_second': 1.332, 'epoch': 3.3632286995515694}\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipython-input-3326690019.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4019\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4020\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4022\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4108\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4109\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4110\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4111\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4112\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     decorate_autocast.__script_unsupported = (  # type: ignore[attr-defined]\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   1860\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Label values must be <= vocab_size: {self.config.vocab_size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1862\u001b[0;31m         outputs = self.wav2vec2(\n\u001b[0m\u001b[1;32m   1863\u001b[0m             \u001b[0minput_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1452\u001b[0m             \u001b[0;31m# compute reduced attention_mask corresponding to feature vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1453\u001b[0;31m             attention_mask = self._get_feature_vector_attention_mask(\n\u001b[0m\u001b[1;32m   1454\u001b[0m                 \u001b[0mextract_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_adapter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m             )\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36m_get_feature_vector_attention_mask\u001b[0;34m(self, feature_vector_length, attention_mask, add_adapter)\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# these two operations makes sure that all values before the output lengths idxs are attended to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mattention_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lengths\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "# training, fine-tuning\n",
                "\n",
                "wer_metric = evaluate.load(\"wer\")\n",
                "cer_metric = evaluate.load(\"cer\")\n",
                "\n",
                "def compute_metrics(pred):\n",
                "    pred_logits = pred.predictions\n",
                "    pred_ids = torch.argmax(torch.tensor(pred_logits), dim=-1)\n",
                "\n",
                "    # decode\n",
                "    pred_str = tokenizer.batch_decode(pred_ids)\n",
                "    label_ids = pred.label_ids\n",
                "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
                "    label_str = tokenizer.batch_decode(label_ids)\n",
                "\n",
                "    print(\"\\n\" + \"=\"*30)\n",
                "    print(f\"SAMPLE 1 TARGET: {label_str[0]}\")\n",
                "    print(f\"SAMPLE 1 PRED:   {pred_str[0]}\")\n",
                "    print(\"-\" * 10)\n",
                "    print(f\"SAMPLE 2 TARGET: {label_str[1]}\")\n",
                "    print(f\"SAMPLE 2 PRED:   {pred_str[1]}\")\n",
                "    print(\"=\"*30 + \"\\n\")\n",
                "\n",
                "    return {\n",
                "        \"wer\": wer_metric.compute(predictions=pred_str, references=label_str),\n",
                "        \"cer\": cer_metric.compute(predictions=pred_str, references=label_str)\n",
                "    }\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./results\",\n",
                "    eval_strategy=\"steps\",\n",
                "    eval_steps=500,\n",
                "    save_strategy=\"steps\",\n",
                "    save_steps=500,\n",
                "    save_total_limit=2,\n",
                "    learning_rate=1e-4,\n",
                "    per_device_train_batch_size=16,\n",
                "    per_device_eval_batch_size=16,\n",
                "    num_train_epochs=10,\n",
                "    warmup_steps=1000,\n",
                "    logging_steps=100,\n",
                "    fp16=torch.cuda.is_available(),\n",
                "    max_grad_norm=1.0,\n",
                "    gradient_accumulation_steps=2,\n",
                "    report_to=[],\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_dataset,\n",
                "    eval_dataset=eval_dataset,\n",
                "    data_collator=data_collator,\n",
                "    compute_metrics=compute_metrics,\n",
                ")\n",
                "\n",
                "trainer.train()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "1HEO5V7BdLPP",
                "outputId": "44b54079-fd19-4278-e786-d0865c13ce9d"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==============================\n",
                        "SAMPLE 1 TARGET: MISTER QUILTER IS THE APOSTLE OF THE MIDLE CLASES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n",
                        "SAMPLE 1 PRED:   MISTER QUILER IS THE OPPOSAL OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n",
                        "----------\n",
                        "SAMPLE 2 TARGET: NOR IS MISTER QUILTER'S MANER LES INTERESTING THAN HIS MATER\n",
                        "SAMPLE 2 PRED:   NOR IS MISTER QUILTER'S MANNER LESS INTERESTING THAN HIS MATTER\n",
                        "==============================\n",
                        "\n",
                        "{'eval_loss': 0.10452383756637573, 'eval_wer': 0.16186904893202456, 'eval_cer': 0.039728733536182406, 'eval_runtime': 127.8705, 'eval_samples_per_second': 21.139, 'eval_steps_per_second': 1.322, 'epoch': 3.368834080717489}\n",
                        "Validation WER: 0.1619\n",
                        "Validation CER: 0.0397\n"
                    ]
                }
            ],
            "source": [
                "# metrics\n",
                "eval_results = trainer.evaluate()\n",
                "print(f\"Validation WER: {eval_results['eval_wer']:.4f}\")\n",
                "print(f\"Validation CER: {eval_results['eval_cer']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "bqYYHMn0eBpH",
                "outputId": "d8f5373c-bf2d-4dc6-8714-3425f6499b99"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 331M/331M [00:19<00:00, 17.7MB/s]\n"
                    ]
                }
            ],
            "source": [
                "test_dataset = LIBRISPEECH(root=root, url=\"test-clean\", download=True)\n",
                "test_dataset = LibriSpeechDataset(test_dataset, tokenizer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "ql3ItyifeMBF",
                "outputId": "5dfea2f1-ae0c-4afd-b5ea-c8ab5e09fdb2"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==============================\n",
                        "SAMPLE 1 TARGET: HE HOPED THERE WOULD BE STEW FOR DINER TURNIPS AND CAROTS AND BRUISED POTATOES AND FAT MUTON PIECES TO BE LADLED OUT IN THICK PEPERED FLOUR FATENED SAUCE\n",
                        "SAMPLE 1 PRED:   HE HOPED THERE WOULD BE STE FOR DINNER TURNIPS AND CARRETS AND BRUISED POTATOES AND FAT MUTTEN PIECES TO BE LAIDLED OUT IN THICK PEPPERED FLOWER FATTINED SAUCE\n",
                        "----------\n",
                        "SAMPLE 2 TARGET: STUF IT INTO YOU HIS BELY COUNSELED HIM\n",
                        "SAMPLE 2 PRED:   STUFF IT INTO YOU HIS BELLY COUNCELED HIM\n",
                        "==============================\n",
                        "\n",
                        "{'eval_loss': 0.11569350212812424, 'eval_wer': 0.16172778454047473, 'eval_cer': 0.03994150310945724, 'eval_runtime': 130.2868, 'eval_samples_per_second': 20.109, 'eval_steps_per_second': 1.259, 'epoch': 3.368834080717489}\n",
                        "Test WER: 0.1617\n",
                        "Test CER: 0.0399\n"
                    ]
                }
            ],
            "source": [
                "test_results = trainer.evaluate(test_dataset)\n",
                "print(f\"Test WER: {test_results['eval_wer']:.4f}\")\n",
                "print(f\"Test CER: {test_results['eval_cer']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trainer.save_model(\"./pretrained_model\")\n",
                "trainer.save_pretrained(\"./pretrained_model\")\n",
                "trainer.save_pretrained(\"./pretrained_model\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "shutil.make_archive('Wav2Vec2-base-LibriSpeech100h', 'zip', \"./pretrained_model\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Domain adaptation: training on custom datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CommandDataset(Dataset):\n",
                "    def __init__(self, folder_path, tokenizer, override_label=None):\n",
                "        self.files = glob.glob(os.path.join(folder_path, \"*.flac\"))\n",
                "        self.tokenizer = tokenizer\n",
                "        self.override_label = override_label\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        file_path = self.files[idx]\n",
                "\n",
                "        if self.override_label:\n",
                "            transcript = self.override_label\n",
                "        else:\n",
                "            filename = os.path.basename(file_path)\n",
                "            clean_name = filename.replace(\".flac\", \"\")\n",
                "            text_part = \"_\".join(clean_name.split(\"_\")[:-1])\n",
                "            text_part = text_part.replace(\"dynamic_\", \"\")\n",
                "            transcript = text_part.replace(\"_\", \" \").upper()\n",
                "\n",
                "        waveform, sr = torchaudio.load(file_path)\n",
                "        \n",
                "        if sr != 16000:\n",
                "            waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
                "\n",
                "        input_values = waveform.squeeze(0).numpy()\n",
                "\n",
                "        return {\"input_values\": input_values, \"labels\": transcript}\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.files)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if not os.path.exists(\"data/raw/custom/commands_dataset\"):\n",
                "    shutil.unpack_archive(\"commands_dataset.zip\", \"data/raw/custom/commands_dataset\")\n",
                "\n",
                "if not os.path.exists(\"data/raw/custom/wakeup_dataset\"):\n",
                "    shutil.unpack_archive(\"wakeup_dataset.zip\", \"data/raw/custom/wakeup_dataset\")\n",
                "\n",
                "if not os.path.exists(\"results/Wav2Vec2-base-LibriSpeech100h\"):\n",
                "    shutil.unpack_archive(\"Wav2Vec2-base-LibriSpeech100h.zip\", \"results/Wav2Vec2-base-LibriSpeech100h\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trained_processor = Wav2Vec2Processor.from_pretrained(\"Wav2Vec2-base-LibriSpeech100h\")\n",
                "trained_tokenizer = trained_processor.tokenizer\n",
                "trained_feature_extractor = trained_processor.feature_extractor\n",
                "\n",
                "trained_model = Wav2Vec2ForCTC.from_pretrained(\"Wav2Vec2-base-LibriSpeech100h\")\n",
                "trained_model.freeze_feature_encoder()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "command_dataset = CommandDataset(\"data/raw/custom/commands_dataset\", trained_tokenizer)\n",
                "wakeup_dataset = CommandDataset(\"data/raw/custom/wakeup_dataset\", trained_tokenizer, override_label=\"WAKE UP TYPIST\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "librispeech_subset = torch.utils.data.Subset(train_ds, torch.randperm(len(train_ds))[:1000])\n",
                "subset_dataset = LibriSpeechDataset(librispeech_subset, trained_tokenizer)\n",
                "\n",
                "train_domain_adapted = ConcatDataset([subset_dataset] + [command_dataset] * 2 + [wakeup_dataset] * 2)\n",
                "print(f\"Domain adapted train dataset length: {len(train_domain_adapted)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "eval_size = int(0.1 * len(train_domain_adapted))\n",
                "train_adapted_ds, eval_adapted_ds = torch.utils.data.random_split(train_domain_adapted, [len(train_domain_adapted)-eval_size, eval_size])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def data_collator_adapted(batch):\n",
                "    audio = [b[\"input_values\"] for b in batch]\n",
                "    text = [b[\"labels\"] for b in batch]\n",
                "\n",
                "    inputs = trained_feature_extractor(\n",
                "        audio,\n",
                "        sampling_rate=16000,\n",
                "        padding=True,\n",
                "        return_attention_mask=True,\n",
                "        return_tensors=\"pt\"\n",
                "    )\n",
                "\n",
                "    labels_batch = trained_tokenizer(\n",
                "        text,\n",
                "        padding=True,\n",
                "        return_tensors=\"pt\",\n",
                "        add_special_tokens=False\n",
                "    )\n",
                "\n",
                "    labels = labels_batch.input_ids\n",
                "    labels[labels == trained_tokenizer.pad_token_id] = -100\n",
                "\n",
                "    return {\n",
                "        \"input_values\": inputs[\"input_values\"],\n",
                "        \"attention_mask\": inputs[\"attention_mask\"],\n",
                "        \"labels\": labels\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wer_metric = evaluate.load(\"wer\")\n",
                "cer_metric = evaluate.load(\"cer\")\n",
                "\n",
                "def compute_metrics_adapted(pred):\n",
                "    pred_logits = pred.predictions\n",
                "    pred_ids = torch.argmax(torch.tensor(pred_logits), dim=-1)\n",
                "\n",
                "    # decode\n",
                "    pred_str = trained_tokenizer.batch_decode(pred_ids)\n",
                "    label_ids = pred.label_ids\n",
                "    label_ids[label_ids == -100] = trained_tokenizer.pad_token_id\n",
                "    label_str = trained_tokenizer.batch_decode(label_ids, group_tokens=False)\n",
                "\n",
                "    print(f\"\\nTARGET: {label_str[0]} \\nPRED: {pred_str[0]}\")\n",
                "\n",
                "    return {\n",
                "        \"wer\": wer_metric.compute(predictions=pred_str, references=label_str),\n",
                "        \"cer\": cer_metric.compute(predictions=pred_str, references=label_str)\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.environ[\"WANDB_DISABLED\"] = \"true\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "training_args_adapted = TrainingArguments(\n",
                "    output_dir=\"./finetuned_results\",\n",
                "    eval_strategy=\"steps\",\n",
                "    eval_steps=100,\n",
                "    save_strategy=\"steps\",\n",
                "    save_steps=100,\n",
                "    learning_rate=2e-5,\n",
                "    per_device_train_batch_size=8,\n",
                "    num_train_epochs=3,\n",
                "    warmup_steps=100,\n",
                "    fp16=torch.cuda.is_available(),\n",
                "    logging_steps=50,\n",
                "    save_total_limit=2,\n",
                ")\n",
                "\n",
                "trainer_adapted = Trainer(\n",
                "    model=trained_model,\n",
                "    args=training_args_adapted,\n",
                "    train_dataset=train_adapted_ds,\n",
                "    eval_dataset=eval_adapted_ds,\n",
                "    data_collator=data_collator_adapted,\n",
                "    compute_metrics=compute_metrics_adapted\n",
                ")\n",
                "\n",
                "trainer_adapted.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trainer_adapted.save_model(\"./final_model\")\n",
                "trained_tokenizer.save_pretrained(\"./final_model\")\n",
                "trained_feature_extractor.save_pretrained(\"./final_model\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "shutil.make_archive('Wav2Vec2-base-LibriSpeech100h-Custom', 'zip', \"./final_model\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
